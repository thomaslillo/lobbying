{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data, Transforming, and Loading into ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "from io import StringIO\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from decouple import config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract Data from Toronto Open Data Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of the files downloaded, but this would ideally go into a log instead if the pipeline is automated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run_filenames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the zip files and moving files into proper folder with dates appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Toronto Open Data is stored in a CKAN instance. It's APIs are documented here:\t\n",
    "# https://docs.ckan.org/en/latest/api/\n",
    "\t\n",
    "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "\n",
    "# Datasets are called \"packages\". Each package can contain many \"resources\"\t\n",
    "# To retrieve the metadata for this package and its resources, use the package name in this page's URL:\n",
    "\t\n",
    "url = base_url + \"/api/3/action/package_show\"\n",
    "params = { \"id\": \"lobbyist-registry\"}\n",
    "package = requests.get(url, params = params).json()\n",
    "\n",
    "# To get resource data:\t\n",
    "for idx, resource in enumerate(package[\"result\"][\"resources\"]):\t\n",
    "    # To get metadata for non datastore_active resources:\n",
    "    if not resource[\"datastore_active\"]:\n",
    "        url = base_url + \"/api/3/action/resource_show?id=\" + resource[\"id\"]\n",
    "        resource_metadata = requests.get(url).json()\n",
    "        if resource_metadata[\"result\"]:\n",
    "            resource_url = resource_metadata[\"result\"][\"url\"]\n",
    "            # Download the resource\n",
    "            response = requests.get(resource_url)\n",
    "            if response.status_code == 200:\n",
    "                # Get the current date to append to the filename\n",
    "                current_date = datetime.now().strftime(\"_%d_%m_%Y\")\n",
    "                \n",
    "                # Open the ZIP file\n",
    "                with zipfile.ZipFile(BytesIO(response.content)) as zip_file:\n",
    "                    # Extract all the contents into the directory\n",
    "                    zip_file.extractall(\"staging/downloaded_files\")\n",
    "                    \n",
    "                    # Rename each file in the ZIP\n",
    "                    for file_info in zip_file.infolist():\n",
    "                        original_filename = file_info.filename\n",
    "                        new_filename = f\"{original_filename.split('.')[0]}{current_date}.{original_filename.split('.')[-1]}\"\n",
    "                        os.rename(f\"staging/downloaded_files/{original_filename}\", f\"staging/downloaded_files/{new_filename}\")\n",
    "                        pipeline_run_filenames.append(new_filename)\n",
    "                break # Break after the first file for demonstration\n",
    "            else:\n",
    "                print(\"Error downloading data, status code: \" + str(response.status_code))\n",
    "                print(response.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transform data from XML format into CSV for easier human review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recursively extract data from nested tags\n",
    "def extract_data(element):\n",
    "    data = {}\n",
    "    for child in element:\n",
    "        # If the child has no children of its own, it is a data tag\n",
    "        if not list(child):\n",
    "            data[child.tag] = child.text\n",
    "        else:\n",
    "            # If the child has children, we recursively extract their data\n",
    "            data.update(extract_data(child))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files from the most recent run of the extract stage that can be used in the transformation stage. This would ideally go into a log instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lobbyactivity-closed_03_11_2023.xml', 'lobbyactivity-active_03_11_2023.xml']\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_run_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Processed:  ./staging/transformed_files/lobbyactivity-closed_03_11_2023.csv\n",
      "File Processed:  ./staging/transformed_files/lobbyactivity-active_03_11_2023.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './staging/downloaded_files/./staging/transformed_files/lobbyactivity-closed_03_11_2023.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Thomas Lillo\\source\\repos\\lobbying\\data\\ETL.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas%20Lillo/source/repos/lobbying/data/ETL.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./staging/downloaded_files/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m filename\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas%20Lillo/source/repos/lobbying/data/ETL.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Reading the content of the file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Thomas%20Lillo/source/repos/lobbying/data/ETL.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Thomas%20Lillo/source/repos/lobbying/data/ETL.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     xml_content \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Thomas%20Lillo/source/repos/lobbying/data/ETL.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Parse the XML content\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Miniconda3\\envs\\hackathon_chroma\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './staging/downloaded_files/./staging/transformed_files/lobbyactivity-closed_03_11_2023.csv'"
     ]
    }
   ],
   "source": [
    "for filename in pipeline_run_filenames:\n",
    "\n",
    "    # Let's read the content of the XML file to understand its structure\n",
    "    file_path = './staging/downloaded_files/' + filename\n",
    "\n",
    "    # Reading the content of the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        xml_content = file.read()\n",
    "\n",
    "    # Parse the XML content\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    # Find all ROW elements\n",
    "    rows = root.findall('.//ROW')\n",
    "\n",
    "    # Extracting the column names (tag names) assuming all rows have the same structure\n",
    "    # and all tags are direct children of the <ROW> tag\n",
    "    column_names = [elem.tag for elem in rows[0]]\n",
    "\n",
    "    # Create a new CSV in-memory file\n",
    "    csv_io = StringIO()\n",
    "    csv_writer = csv.writer(csv_io)\n",
    "\n",
    "    # Initialize an empty list for column names\n",
    "    column_names = []\n",
    "\n",
    "    # Processing rows to extract column names and data\n",
    "    all_row_data = []\n",
    "\n",
    "    for row in rows:\n",
    "        # Extract data from each <SM> tag, which contains the relevant data tags\n",
    "        sm_elements = row.findall('.//SM')\n",
    "        # Collect data from all <SM> elements (assuming there might be more than one <SM> per <ROW>)\n",
    "        row_data = {}\n",
    "        for sm in sm_elements:\n",
    "            row_data.update(extract_data(sm))\n",
    "\n",
    "        all_row_data.append(row_data)\n",
    "        # Update the column names with any new unique tags found\n",
    "        column_names.extend([key for key in row_data.keys() if key not in column_names])\n",
    "\n",
    "    # Write the column names as the header\n",
    "    csv_writer.writerow(column_names)\n",
    "\n",
    "    # Write the data to the CSV, ensuring each row has data in the same order as the column names\n",
    "    for row_data in all_row_data:\n",
    "        csv_writer.writerow([row_data.get(col, None) for col in column_names])\n",
    "\n",
    "    # Retrieve the CSV content\n",
    "    csv_content = csv_io.getvalue()\n",
    "\n",
    "    # Save the CSV content to a file\n",
    "    csv_file_path = './staging/transformed_files/'+str(filename[:-4])+'.csv'\n",
    "    with open(csv_file_path, 'w') as csvfile:\n",
    "        csvfile.write(csv_content)\n",
    "\n",
    "    csv_file_path  # Return the path of the created CSV file\n",
    "    \n",
    "    pipeline_run_filenames.append(csv_file_path)\n",
    "    print(\"File Processed: \", csv_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load data into chromaDB, the vector database being used to support semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extablish connection to database and create collection object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the API key\n",
    "api_key = config('OPEN_API_KEY')\n",
    "\n",
    "# set up the client connection to the docker container server\n",
    "chroma_client = chromadb.HttpClient(host='localhost', port='8000')\n",
    "\n",
    "print(\"db heartbeat: \")\n",
    "print(chroma_client.heartbeat()) # returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"lobbying_metadata\", embedding_function=openai_ef, metadata={\"hnsw:space\": \"cosine\"}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in each csv file into a pandas dataframe and write to database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./staging/transformed_files/lobbyactivity-closed_03_11_2023.csv\n",
      "./staging/transformed_files/lobbyactivity-active_03_11_2023.csv\n"
     ]
    }
   ],
   "source": [
    "for pipeline_file in pipeline_run_filenames:\n",
    "    if pipeline_file[-4:] == \".csv\":\n",
    "        print(pipeline_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the files from a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "metadatas = []\n",
    "ids = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through the dataframe and add elements to the different collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    r_document = row['VARIABLEDESC']\n",
    "    r_metadata = {}\n",
    "    r_metadata['COUNTRY'] = row['COUNTRY']\n",
    "    r_metadata['DATASETNAME'] = row['DATASETNAME']\n",
    "    r_metadata['VINTAGE'] = row['VINTAGE']\n",
    "    r_metadata['VARIABLENAME'] = row['VARIABLENAME']\n",
    "    r_id = row['UniqueId']\n",
    "\n",
    "    documents.append(r_document)\n",
    "    metadatas.append(r_metadata)\n",
    "    ids.append(r_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the values in each list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "collection = chroma_client.get_or_create_collection(name=\"lobbying_metadata\", embedding_function=openai_ef, metadata={\"hnsw:space\": \"cosine\"}) \n",
    "\n",
    "### WRITING THE DATA ==================================================\n",
    "\n",
    "# use batches of 1000 so as not to overwhelm the OpenAI API\n",
    "batch_size = 1000\n",
    "num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "for batch_num in range(num_batches):\n",
    "    print(\"on batch\" + str(batch_num) + \" of \" + str(num_batches) + \"...\")\n",
    "    start_index = batch_num * batch_size\n",
    "    end_index = min((batch_num + 1) * batch_size, len(df))\n",
    "    batch = df.iloc[start_index:end_index]\n",
    "\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    for index, row in batch.iterrows():\n",
    "        r_document = row['VARIABLEDESC']\n",
    "        r_metadata = {}\n",
    "        r_metadata['COUNTRY'] = row['COUNTRY']\n",
    "        r_metadata['DATASETNAME'] = row['DATASETNAME']\n",
    "        r_metadata['VINTAGE'] = row['VINTAGE']\n",
    "        r_metadata['VARIABLENAME'] = row['VARIABLENAME']\n",
    "        r_id = str(row['UniqueId'])\n",
    "        documents.append(r_document)\n",
    "        metadatas.append(r_metadata)\n",
    "        ids.append(r_id)\n",
    "\n",
    "    try:\n",
    "        collection.upsert(\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )  \n",
    "    except:\n",
    "        print(\"\\nError in batch: \" + str(batch_num))\n",
    "        print(\"IDs starting at \" + str(start_index) + \" and ending at \" + str(end_index) + \"... \\n\")\n",
    "\n",
    "print(\"\\n\\n...done writing data!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
